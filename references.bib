@inproceedings{akiba2019optuna,
  author    = {Akiba, Takuya and Sano, Shotaro and Yanase, Takeru and Ohta, Toshihiko and Koyama, Masanori},
  title     = {Optuna: A Next-generation Hyperparameter Optimization Framework},
  booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages     = {2623--2631},
  year      = {2019},
  publisher = {ACM},
  doi       = {10.1145/3292500.3330701},
  url       = {https://doi.org/10.1145/3292500.3330701}
}

@article{louhichi2023shapley,
  author    = {Mouad Louhichi and Redwane Nesmaoui and Marwan Mbarek and Mohamed Lazaar},
  title     = {Shapley Values for Explaining the Black Box Nature of Machine Learning Model Clustering},
  journal   = {Procedia Computer Science},
  volume    = {220},
  pages     = {806--811},
  year      = {2023},
  issn      = {1877-0509},
  doi       = {10.1016/j.procs.2023.03.107},
  url       = {https://doi.org/10.1016/j.procs.2023.03.107}
}

@article{patton2009evaluating,
  title={Evaluating volatility and correlation forecasts},
  author={Patton, Andrew J. and Sheppard, Kevin},
  journal={Handbook of Financial Time Series},
  pages={801--838},
  year={2009},
  publisher={Springer},
  doi={10.1007/978-3-540-71297-8_32}
}

@article{rmse-mae,
author = {Chai, Tianfeng and Draxler, R.},
year = {2014},
month = {01},
pages = {},
title = {Root mean square error (RMSE) or mean absolute error (MAE)?},
volume = {7},
journal = {Geosci. Model Dev.},
doi = {10.5194/gmdd-7-1525-2014}
}

@article{mackenzie2024traders,
  author    = {Mackenzie, Nell},
  title     = {Traders lose billions on big volatility short after stocks rout},
  journal   = {Reuters},
  year      = {2024},
  month     = {August 8},
  url       = {https://www.reuters.com/markets/us/traders-lose-billions-big-volatility-short-after-stocks-rout-2024-08-07/},
  note      = {Accessed: 2025-05-31}
}

@article{nelson1991conditional,
  title={Conditional heteroskedasticity in asset returns: A new approach},
  author={Nelson, Daniel B},
  journal={Econometrica: Journal of the econometric society},
  pages={347--370},
  year={1991},
  publisher={JSTOR}
}

@book{abergel2016high,
  title={High-Frequency Trading and the Emergence of a New Financial Ecosystem},
  author={Abergel, Fr{\'e}d{\'e}ric and Chakrabarti, Bikas K. and Chakraborti, Anirban and Ghosh, Abhijit},
  year={2016},
  publisher={Cambridge University Press},
  doi={10.1017/CBO9781316676536},
  url={https://doi.org/10.1017/CBO9781316676536}
}


@article{mohammad2020dynamical,
  title={Dynamical approach in studying stability condition of exponential (GARCH) models},
  author={Mohammad, A. A. and Mudhir, A. A.},
  journal={Journal of King Saud University - Science},
  volume={32},
  number={1},
  pages={272--278},
  year={2020},
  publisher={Elsevier},
  doi={10.1016/j.jksus.2018.04.028},
  url={https://doi.org/10.1016/j.jksus.2018.04.028}
}

@article{bhambu2025high,
  title={High frequency volatility forecasting and risk assessment using neural networks-based heteroscedasticity model},
  author={Bhambu, Aryan and Bera, Koushik and Natarajan, Selvaraju and Suganthan, Ponnuthurai Nagaratnam},
  journal={Engineering Applications of Artificial Intelligence},
  volume={149},
  pages={110397},
  year={2025},
  publisher={Elsevier},
  doi={10.1016/j.engappai.2025.110397}
}

@inproceedings{ke2017,
author = {Ke, Guolin and Meng, Qi and Finley, Thomas and Wang, Taifeng and Chen, Wei and Ma, Weidong and Ye, Qiwei and Liu, Tie-Yan},
title = {LightGBM: a highly efficient gradient boosting decision tree},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Gradient Boosting Decision Tree (GBDT) is a popular machine learning algorithm, and has quite a few effective implementations such as XGBoost and pGBRT. Although many engineering optimizations have been adopted in these implementations, the efficiency and scalability are still unsatisfactory when the feature dimension is high and data size is large. A major reason is that for each feature, they need to scan all the data instances to estimate the information gain of all possible split points, which is very time consuming. To tackle this problem, we propose two novel techniques: Gradient-based One-Side Sampling (GOSS) and Exclusive Feature Bundling (EFB). With GOSS, we exclude a significant proportion of data instances with small gradients, and only use the rest to estimate the information gain. We prove that, since the data instances with larger gradients play a more important role in the computation of information gain, GOSS can obtain quite accurate estimation of the information gain with a much smaller data size. With EFB, we bundle mutually exclusive features (i.e., they rarely take nonzero values simultaneously), to reduce the number of features. We prove that finding the optimal bundling of exclusive features is NP-hard, but a greedy algorithm can achieve quite good approximation ratio (and thus can effectively reduce the number of features without hurting the accuracy of split point determination by much). We call our new GBDT implementation with GOSS and EFB LightGBM. Our experiments on multiple public datasets show that, LightGBM speeds up the training process of conventional GBDT by up to over 20 times while achieving almost the same accuracy.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {3149–3157},
numpages = {9},
location = {Long Beach, California, USA},
series = {NIPS'17}
}

@inproceedings{chen2016,
author = {Chen, Tianqi and Guestrin, Carlos},
title = {XGBoost: A Scalable Tree Boosting System},
year = {2016},
isbn = {9781450342322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2939672.2939785},
doi = {10.1145/2939672.2939785},
abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {785–794},
numpages = {10},
keywords = {large-scale machine learning},
location = {San Francisco, California, USA},
series = {KDD '16}
}

@article{breiman2001random,
  title={Random Forests},
  author={Breiman, Leo},
  journal={Machine Learning},
  volume={45},
  number={1},
  pages={5--32},
  year={2001},
  publisher={Springer},
  doi={10.1023/A:1010933404324},
  url={https://doi.org/10.1023/A:1010933404324}
}

@article{feng2022statistical,
  title={A statistical learning assessment of Huber regression},
  author={Feng, Yunlong and Wu, Qiang},
  journal={Journal of Approximation Theory},
  volume={273},
  pages={105660},
  year={2022},
  issn={0021-9045},
  doi={10.1016/j.jat.2021.105660},
  url={https://doi.org/10.1016/j.jat.2021.105660}
}
